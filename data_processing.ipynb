{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bb946870",
   "metadata": {},
   "source": [
    "# Homework 1: Multi-Task Learning with a Two-Headed MLP\n",
    "\n",
    "In this notebook, we implement a multi-task learning model using a two-headed MLP.  \n",
    "The model predicts:\n",
    "\n",
    "1. **Student's final grade (G3)** — regression task  \n",
    "2. **Student's romantic status** — classification task ('yes' or 'no')  \n",
    "\n",
    "We preprocess the UCI Student Performance dataset (specifically student-por.csv), train a shared MLP body with two heads, and evaluate the model on both tasks using appropriate metrics (MAE, Accuracy, F1-Score).  \n",
    "We also experiment with **weighted losses** to balance the regression and classification objectives."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25c04738",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports necessary for model training and evaluation\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import Dataset\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.metrics import accuracy_score, f1_score, mean_absolute_error\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Relative file path to desired dataset\n",
    "CVS_PATH = 'student/student-por.csv'\n",
    "\n",
    "# Reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c789b848",
   "metadata": {},
   "source": [
    "## Data Preprocessing Setup\n",
    "\n",
    "Before feeding data into the neural network, we need to distinguish between **categorical** and **numerical** features:\n",
    "\n",
    "- **Categorical (nominal) features**: These include binary or multi-class columns like `sex`, `school`, `Mjob`, etc.  \n",
    "  We will convert them into **one-hot encoded vectors** so that the network can process them as numerical inputs.\n",
    "\n",
    "- **Numerical features**: Columns like `age`, `G1`, `G2`, etc. will be **normalized** using `StandardScaler`.  \n",
    "  Normalization ensures that all numeric inputs are on a similar scale, which improves training stability and convergence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ccc0732",
   "metadata": {},
   "outputs": [],
   "source": [
    "# These are nominal columns that will be transformed into one-hot vectors\n",
    "COLUMNS_TO_CATEGORIZE = ['school', 'sex', 'address', 'famsize', 'Pstatus', 'Mjob', 'Fjob', 'reason', 'guardian', 'schoolsup', 'famsup', 'paid',\n",
    "                         'activities', 'nursery', 'higher', 'internet', 'romantic']\n",
    "\n",
    "# These are nominal columns that will be scaled\n",
    "COLUMNS_TO_NORMALIZE = ['age', 'Medu', 'Fedu', 'traveltime', 'studytime', 'failures', 'famrel', 'freetime', 'goout', 'Dalc', 'Walc', 'health', 'absences', 'G1', 'G2'] "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11ffcf76",
   "metadata": {},
   "source": [
    "## Load and Preprocess Data\n",
    "\n",
    "In this step, we will:\n",
    "\n",
    "1. **Load the CSV dataset** into a pandas DataFrame.  \n",
    "2. **Apply one-hot encoding** to the categorical columns defined earlier.  \n",
    "3. **Normalize numerical columns** using `StandardScaler` to ensure all numeric features are on a similar scale.  \n",
    "\n",
    "After preprocessing, the data will be ready to split into inputs (`X`) and targets (`y_grade` and `y_romantic`) for training the multi-task model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1d5c7b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading data from file\n",
    "df = pd.read_csv(CVS_PATH, delimiter=';')\n",
    "\n",
    "# This does one-hot encoding (for each column each category we will have column (e.g. sex_M, sex_F) that has either 0 or 1 value)\n",
    "df = pd.get_dummies(\n",
    "    df, \n",
    "    columns=COLUMNS_TO_CATEGORIZE,\n",
    "    prefix=COLUMNS_TO_CATEGORIZE\n",
    ").astype(int)\n",
    "\n",
    "# We use StandardScaler here, nothing special\n",
    "scaler = StandardScaler()\n",
    "df[COLUMNS_TO_NORMALIZE] = scaler.fit_transform(df[COLUMNS_TO_NORMALIZE])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7378ab5",
   "metadata": {},
   "source": [
    "Separate the features and targets:  \n",
    "- `X` contains all input features.  \n",
    "- `y_grade` is the regression target (final grade G3).  \n",
    "- `y_romantic` is the classification target (romantic status, 0/1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab0084a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X = df.drop(columns=['G3', 'romantic_yes', 'romantic_no']).values.astype(np.float32)\n",
    "y_grade = df['G3'].values.astype(np.float32).reshape(-1, 1)   # type: ignore\n",
    "y_romantic = df['romantic_yes'].values.astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e658298",
   "metadata": {},
   "source": [
    "Split the data into **train, validation, and test sets**:  \n",
    "- The first split separates out the test set (15% of the data).  \n",
    "- The second split divides the remaining data into training and validation sets (≈75% train, ≈15% validation).  \n",
    "This ensures we can train the model, validate during training, and evaluate on a held-out test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c065f88",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_trainval, X_test, y_grade_trainval, y_grade_test, y_rom_trainval, y_rom_test = train_test_split(\n",
    "    X, y_grade, y_romantic, test_size=0.15, random_state=42\n",
    ")\n",
    "X_train, X_val, y_grade_train, y_grade_val, y_rom_train, y_rom_val = train_test_split(\n",
    "    X_trainval, y_grade_trainval, y_rom_trainval, test_size=0.15, random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "115a9fc5",
   "metadata": {},
   "source": [
    "Create a **custom PyTorch Dataset** to handle our multi-task data:  \n",
    "\n",
    "- Returns a tuple `(x_data, y_grade, y_romantic)` for each sample.  \n",
    "- Ensures features are `float32` and targets are the correct types (`float32` for regression, `long` for classification).  \n",
    "- Works seamlessly with a `DataLoader` for batching."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "857e3ec3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class StudentDatasetPor(Dataset):\n",
    "    def __init__(self, X, y_grade, y_romantic):\n",
    "        self.X = torch.tensor(X, dtype=torch.float32)\n",
    "        self.y_grade = torch.tensor(y_grade, dtype=torch.float32)\n",
    "        self.y_romantic = torch.tensor(y_romantic, dtype=torch.long)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y_grade[idx], self.y_romantic[idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99cc55c2",
   "metadata": {},
   "source": [
    "Instantiate the **train, validation, and test datasets** using our custom `StudentDatasetPor` class.  \n",
    "These datasets will later be fed into `DataLoader`s for batching during training and evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fa8cdd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = StudentDatasetPor(X_train, y_grade_train, y_rom_train)\n",
    "val_dataset = StudentDatasetPor(X_val, y_grade_val, y_rom_val)\n",
    "test_dataset = StudentDatasetPor(X_test, y_grade_test, y_rom_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3fb942c",
   "metadata": {},
   "source": [
    "Define **training hyperparameters** and **network layer sizes**:  \n",
    "\n",
    "- `BATCH_SIZE`: Number of samples per batch  \n",
    "- `FIRST_NEURON_N`, `SECOND_NEURON_N`, `THIRD_NEURON_N`: Sizes of hidden layers in the shared MLP body  \n",
    "- `LEARNING_RATE`: Learning rate for the optimizer  \n",
    "- `EPOCHS`: Number of training iterations over the full dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b1bed4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 16\n",
    "FIRST_NEURON_N = 16\n",
    "SECOND_NEURON_N = 8\n",
    "LEARNING_RATE = 1e-3\n",
    "EPOCHS = 50"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "465847a7",
   "metadata": {},
   "source": [
    "Create **DataLoaders** to efficiently feed data into the model:  \n",
    "\n",
    "- `train_loader` shuffles the training data each epoch for better generalization.  \n",
    "- `val_loader` and `test_loader` do not shuffle, ensuring consistent evaluation.  \n",
    "- `BATCH_SIZE` controls how many samples are processed in each iteration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d86d264e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38f9d329",
   "metadata": {},
   "source": [
    "Define the **Multi-Task Neural Network Architecture**:\n",
    "\n",
    "- The model consists of a **shared MLP body** that learns a common representation of the student profile.\n",
    "- Two separate **task-specific heads** are built on top of the shared features:\n",
    "  - **Grade Head (Regression):** Outputs a single value for the predicted final grade.\n",
    "  - **Romantic Head (Classification):** Outputs logits for the two classes (yes / no).\n",
    "- The grade output is scaled to the range **[0, 20]** using a `sigmoid` activation.\n",
    "- Batch Normalization and Dropout are used for **training stability and regularization**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a093324a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class StudentMLP(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.shared = nn.Sequential(\n",
    "            nn.Linear(input_dim, FIRST_NEURON_N),\n",
    "            nn.BatchNorm1d(FIRST_NEURON_N),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(FIRST_NEURON_N, SECOND_NEURON_N),\n",
    "            nn.BatchNorm1d(SECOND_NEURON_N),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2)\n",
    "        )\n",
    "        \n",
    "        self.grade_head = nn.Linear(SECOND_NEURON_N, 1)\n",
    "        \n",
    "        self.romantic_head = nn.Linear(SECOND_NEURON_N, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        features = self.shared(x)\n",
    "\n",
    "        grade_pred = torch.sigmoid(self.grade_head(features)) * 20\n",
    "        romantic_logit = self.romantic_head(features)\n",
    "\n",
    "        return grade_pred, romantic_logit"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89cf2e86",
   "metadata": {},
   "source": [
    "Set up the **training environment**:\n",
    "\n",
    "- Select the computation device (GPU if available, otherwise CPU).\n",
    "- Initialize the multi-task model and move it to the chosen device.\n",
    "- Define the two loss functions:\n",
    "  - `MSELoss` for grade prediction (regression)\n",
    "  - `CrossEntropyLoss` for romantic status prediction (classification)\n",
    "- Initialize the optimizer (`Adam`) with the chosen learning rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a823264",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model = StudentMLP(X.shape[1]).to(device)\n",
    "\n",
    "criterion_grade = nn.MSELoss()               \n",
    "\n",
    "y_rom_train_np = y_rom_train.numpy() if isinstance(y_rom_train, torch.Tensor) else y_rom_train\n",
    "\n",
    "class_weights = compute_class_weight(\n",
    "    class_weight='balanced',\n",
    "    classes=np.array([0, 1]),\n",
    "    y=y_rom_train_np\n",
    ")\n",
    "\n",
    "class_weights = torch.tensor(class_weights, dtype=torch.float32).to(device)\n",
    "\n",
    "criterion_romantic = nn.CrossEntropyLoss(weight=class_weights)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a5e8623",
   "metadata": {},
   "source": [
    "Initialize lists to **store training and validation losses** for later visualization:\n",
    "\n",
    "- Total loss\n",
    "- Grade (regression) loss\n",
    "- Romantic (classification) loss\n",
    "\n",
    "These will be used to plot learning curves after training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbbb5226",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_losses = []\n",
    "val_losses = []\n",
    "train_grade_losses = []\n",
    "val_grade_losses = []\n",
    "train_romantic_losses = []\n",
    "val_romantic_losses = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2fcf797",
   "metadata": {},
   "source": [
    "Define a helper function to compute the **weighted total loss** using a mixing coefficient `alpha`:\n",
    "\n",
    "- `alpha` controls the importance of the **grade regression loss**\n",
    "- `(1 - alpha)` controls the importance of the **romantic classification loss**\n",
    "\n",
    "This allows flexible balancing between the two tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6135884a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_loss_alpha(loss_grade, loss_romantic, alpha):\n",
    "    return loss_grade*alpha + (1-alpha)*loss_romantic"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cf4e5a2",
   "metadata": {},
   "source": [
    "Train and validate the **multi-task model** for one full training cycle over `EPOCHS`.\n",
    "\n",
    "At each epoch:\n",
    "- The model is trained on the **training set**\n",
    "- Then evaluated on the **validation set**\n",
    "- Both **grade regression loss** and **romantic classification loss** are tracked\n",
    "- A combined loss is computed using `alpha`:\n",
    "  - If `alpha = None`, losses are simply summed\n",
    "  - Otherwise, a weighted combination is used\n",
    "\n",
    "All losses are stored for later visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68e57cf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_val_model(alpha = None): \n",
    "    for epoch in range(EPOCHS):\n",
    "        model.train()\n",
    "        total_train_loss = 0\n",
    "        total_train_grade_loss = 0\n",
    "        total_train_romantic_loss = 0\n",
    "\n",
    "        for x_batch, y_grade_batch, y_romantic_batch in train_loader:\n",
    "            x_batch = x_batch.to(device)\n",
    "            y_grade_batch = y_grade_batch.to(device)\n",
    "            y_romantic_batch = y_romantic_batch.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            grade_pred, romantic_logits = model(x_batch)\n",
    "\n",
    "            loss_grade = criterion_grade(grade_pred, y_grade_batch)\n",
    "            loss_romantic = criterion_romantic(romantic_logits, y_romantic_batch)\n",
    "\n",
    "            if alpha is None:\n",
    "                loss = loss_grade + loss_romantic\n",
    "            else:\n",
    "                loss = calculate_loss_alpha(loss_grade=loss_grade, loss_romantic=loss_romantic, alpha=alpha)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_train_loss += loss.item() * x_batch.size(0)\n",
    "            total_train_grade_loss += loss_grade.item() * x_batch.size(0)\n",
    "            total_train_romantic_loss += loss_romantic.item() * x_batch.size(0)\n",
    "\n",
    "        total_train_loss /= len(train_loader.dataset) # type: ignore\n",
    "        total_train_grade_loss /= len(train_loader.dataset) # type: ignore\n",
    "        total_train_romantic_loss /= len(train_loader.dataset) # type: ignore\n",
    "        train_losses.append(total_train_loss)\n",
    "        train_grade_losses.append(total_train_grade_loss)\n",
    "        train_romantic_losses.append(total_train_romantic_loss)\n",
    "\n",
    "        # =======================\n",
    "        # VALIDATION\n",
    "        # =======================\n",
    "        model.eval()\n",
    "        total_val_loss = 0\n",
    "        total_val_grade_loss = 0\n",
    "        total_val_romantic_loss = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for x_batch, y_grade_batch, y_romantic_batch in val_loader:\n",
    "                x_batch = x_batch.to(device)\n",
    "                y_grade_batch = y_grade_batch.to(device)\n",
    "                y_romantic_batch = y_romantic_batch.to(device)\n",
    "\n",
    "                grade_pred, romantic_logits = model(x_batch)\n",
    "\n",
    "                loss_grade = criterion_grade(grade_pred, y_grade_batch)\n",
    "                loss_romantic = criterion_romantic(romantic_logits, y_romantic_batch)\n",
    "\n",
    "                if alpha is None:\n",
    "                    loss_val = loss_grade + loss_romantic\n",
    "                else:\n",
    "                    loss_val = calculate_loss_alpha(loss_grade, loss_romantic, alpha)\n",
    "\n",
    "                total_val_loss += loss_val.item() * x_batch.size(0)\n",
    "                total_val_grade_loss += loss_grade.item() * x_batch.size(0)\n",
    "                total_val_romantic_loss += loss_romantic.item() * x_batch.size(0)\n",
    "\n",
    "        total_val_loss /= len(val_loader.dataset) # type: ignore\n",
    "        total_val_grade_loss /= len(val_loader.dataset) # type: ignore\n",
    "        total_val_romantic_loss /= len(val_loader.dataset) # type: ignore\n",
    "        val_losses.append(total_val_loss)\n",
    "        val_grade_losses.append(total_val_grade_loss)\n",
    "        val_romantic_losses.append(total_val_romantic_loss)\n",
    "\n",
    "        # print(\n",
    "        #     f\"Epoch [{epoch+1}/{EPOCHS}] | \"\n",
    "        #     f\"Train Loss: {total_train_loss:.4f} | \"\n",
    "        #     f\"Val Loss: {total_val_loss:.4f}\"\n",
    "        # )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4ae1ae7",
   "metadata": {},
   "source": [
    "Evaluate the trained model on the **test set** using task-specific metrics:\n",
    "\n",
    "- **MAE** for grade regression\n",
    "- **Accuracy** for romantic relationship classification\n",
    "- **F1-score (Yes class)** to measure positive class quality\n",
    "\n",
    "Predictions are collected batch-by-batch with `torch.no_grad()` to disable gradient tracking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fb54105",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_test(model, test_loader, device):\n",
    "    model.eval()\n",
    "\n",
    "    all_grade_preds = []\n",
    "    all_grade_true = []\n",
    "\n",
    "    all_romantic_preds = []\n",
    "    all_romantic_true = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for x_batch, y_grade_batch, y_romantic_batch in test_loader:\n",
    "            x_batch = x_batch.to(device)\n",
    "            y_grade_batch = y_grade_batch.to(device)\n",
    "            y_romantic_batch = y_romantic_batch.to(device)\n",
    "\n",
    "            grade_out, romantic_out = model(x_batch)\n",
    "\n",
    "            all_grade_preds.extend(grade_out.cpu().numpy().flatten())\n",
    "            all_grade_true.extend(y_grade_batch.cpu().numpy().flatten())\n",
    "\n",
    "            romantic_pred_labels = romantic_out.argmax(dim=1)\n",
    "            all_romantic_preds.extend(romantic_pred_labels.cpu().numpy())\n",
    "            all_romantic_true.extend(y_romantic_batch.cpu().numpy())\n",
    "\n",
    "\n",
    "    mae = mean_absolute_error(all_grade_true, all_grade_preds)\n",
    "\n",
    "    accuracy = accuracy_score(all_romantic_true, all_romantic_preds)\n",
    "\n",
    "    f1_yes = f1_score(all_romantic_true, all_romantic_preds, pos_label=1)\n",
    "\n",
    "    return {\n",
    "        \"grade_MAE\": mae,\n",
    "        \"romantic_accuracy\": accuracy,\n",
    "        \"romantic_f1_yes\": f1_yes\n",
    "    }\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6013de8",
   "metadata": {},
   "source": [
    "This is **experiment** function that you can use to test model with different alphas (or without even alpha):\n",
    "\n",
    "- Resets **model and optimizer**\n",
    "- Resets all **history**\n",
    "- Trains model with given **alpha**\n",
    "- Tests **trained model** on test data loader and prints results\n",
    "- Saves model if you give **save_name**\n",
    "- Plots all history of **losses**\n",
    "\n",
    "You can use this function under the block where function is defined."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b650567",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_experiment(alpha=None, save_name=None):\n",
    "    global model, optimizer\n",
    "    \n",
    "    # ============================\n",
    "    # 1. RESET MODEL + OPTIMIZER\n",
    "    # ============================\n",
    "    model = StudentMLP(X.shape[1]).to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "    # ============================\n",
    "    # 2. RESET ALL HISTORY\n",
    "    # ============================\n",
    "    train_losses.clear()\n",
    "    val_losses.clear()\n",
    "    train_grade_losses.clear()\n",
    "    val_grade_losses.clear()\n",
    "    train_romantic_losses.clear()\n",
    "    val_romantic_losses.clear()\n",
    "\n",
    "    # ============================\n",
    "    # 3. TRAIN + VALIDATE\n",
    "    # ============================\n",
    "    train_val_model(alpha)\n",
    "\n",
    "    # ============================\n",
    "    # 4. TEST EVALUATION\n",
    "    # ============================\n",
    "    results = evaluate_test(model, test_loader, device)\n",
    "\n",
    "    print(\"\\n===== TEST RESULTS =====\")\n",
    "    print(f\"Grade MAE          : {results['grade_MAE']:.4f}\")\n",
    "    print(f\"Romantic Accuracy  : {results['romantic_accuracy']:.4f}\")\n",
    "    print(f\"Romantic F1 (Yes) : {results['romantic_f1_yes']:.4f}\")\n",
    "\n",
    "    # ============================\n",
    "    # 5. SAVE MODEL (OPTIONAL)\n",
    "    # ============================\n",
    "    if save_name:\n",
    "        torch.save(model.state_dict(), save_name)\n",
    "        print(f\"\\nModel saved as: {save_name}\")\n",
    "\n",
    "    # ============================\n",
    "    # 6. PLOTTING\n",
    "    # ============================\n",
    "    epochs_range = range(1, EPOCHS + 1)\n",
    "\n",
    "    plt.figure()\n",
    "    plt.plot(epochs_range, train_losses, label=\"Train Total Loss\")\n",
    "    plt.plot(epochs_range, val_losses, label=\"Val Total Loss\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.title(\"Total Loss\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "    plt.figure()\n",
    "    plt.plot(epochs_range, train_grade_losses, label=\"Train Grade Loss\")\n",
    "    plt.plot(epochs_range, val_grade_losses, label=\"Val Grade Loss\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"MSE\")\n",
    "    plt.title(\"Grade Loss\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "    plt.figure()\n",
    "    plt.plot(epochs_range, train_romantic_losses, label=\"Train Romantic Loss\")\n",
    "    plt.plot(epochs_range, val_romantic_losses, label=\"Val Romantic Loss\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Cross-Entropy\")\n",
    "    plt.title(\"Romantic Loss\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f913b639",
   "metadata": {},
   "source": [
    "## Basic Model's Results\n",
    "===== TEST RESULTS =====\n",
    "- Grade MAE          : 0.8422\n",
    "- Romantic Accuracy  : 0.5204\n",
    "- Romantic F1 (Yes) : 0.3188"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e460877",
   "metadata": {},
   "source": [
    "Test Here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6317ee4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For example (alpha = None, total_loss = loss_grade + loss_romantic)\n",
    "run_experiment(save_name='basic_model.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ba2afcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_large = run_experiment(alpha=0.8, save_name=\"large_alpha_model.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6dc7b12",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_medium = run_experiment(alpha=0.5, save_name=\"medium_alpha_model.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3764b3cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_small = run_experiment(alpha=0.2, save_name='small_alpha_model.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd3bedb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_extra_small = run_experiment(alpha=0.01, save_name='extra_small_alpha_model.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91f56a8f",
   "metadata": {},
   "source": [
    "### Bonus Task Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff1c2e3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_dict = {\n",
    "    0.01: results_extra_small,\n",
    "    0.2: results_small,\n",
    "    0.5: results_medium,\n",
    "    0.8: results_large\n",
    "}\n",
    "\n",
    "\n",
    "table = pd.DataFrame({\n",
    "    'Alpha': list(results_dict.keys()),\n",
    "    'Grade MAE': [res['grade_MAE'] for res in results_dict.values()],\n",
    "    'Romantic Accuracy': [res['romantic_accuracy'] for res in results_dict.values()],\n",
    "    'Romantic F1 (Yes)': [res['romantic_f1_yes'] for res in results_dict.values()]\n",
    "})\n",
    "\n",
    "table = table.sort_values('Alpha').reset_index(drop=True)\n",
    "\n",
    "print(table)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bb946870",
   "metadata": {},
   "source": [
    "# Homework 1: Multi-Task Learning with a Two-Headed MLP\n",
    "\n",
    "In this notebook, we implement a multi-task learning model using a two-headed MLP.  \n",
    "The model predicts:\n",
    "\n",
    "1. **Student's final grade (G3)** — regression task  \n",
    "2. **Student's romantic status** — classification task ('yes' or 'no')  \n",
    "\n",
    "We preprocess the UCI Student Performance dataset (specifically student-por.csv), train a shared MLP body with two heads, and evaluate the model on both tasks using appropriate metrics (MAE, Accuracy, F1-Score).  \n",
    "We also experiment with **weighted losses** to balance the regression and classification objectives."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25c04738",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports necessary for model training and evaluation\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import Dataset\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.metrics import accuracy_score, f1_score, mean_absolute_error\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Relative file path to desired dataset\n",
    "CVS_PATH = 'student/student-por.csv'\n",
    "\n",
    "# Reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c789b848",
   "metadata": {},
   "source": [
    "## Data Preprocessing Setup\n",
    "\n",
    "Before feeding data into the neural network, we need to distinguish between **categorical** and **numerical** features:\n",
    "\n",
    "- **Categorical (nominal) features**: These include binary or multi-class columns like `sex`, `school`, `Mjob`, etc.  \n",
    "  We will convert them into **one-hot encoded vectors** so that the network can process them as numerical inputs.\n",
    "\n",
    "- **Numerical features**: Columns like `age`, `G1`, `G2`, etc. will be **normalized** using `StandardScaler`.  \n",
    "  Normalization ensures that all numeric inputs are on a similar scale, which improves training stability and convergence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ccc0732",
   "metadata": {},
   "outputs": [],
   "source": [
    "# These are nominal columns that will be transformed into one-hot vectors\n",
    "COLUMNS_TO_CATEGORIZE = ['school', 'sex', 'address', 'famsize', 'Pstatus', 'Mjob', 'Fjob', 'reason', 'guardian', 'schoolsup', 'famsup', 'paid',\n",
    "                         'activities', 'nursery', 'higher', 'internet', 'romantic']\n",
    "\n",
    "# These are nominal columns that will be scaled\n",
    "COLUMNS_TO_NORMALIZE = ['age', 'Medu', 'Fedu', 'traveltime', 'studytime', 'failures', 'famrel', 'freetime', 'goout', 'Dalc', 'Walc', 'health', 'absences', 'G1', 'G2'] "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11ffcf76",
   "metadata": {},
   "source": [
    "## Load and Preprocess Data\n",
    "\n",
    "In this step, we will:\n",
    "\n",
    "1. **Load the CSV dataset** into a pandas DataFrame.  \n",
    "2. **Apply one-hot encoding** to the categorical columns defined earlier.  \n",
    "3. **Normalize numerical columns** using `StandardScaler` to ensure all numeric features are on a similar scale.  \n",
    "\n",
    "After preprocessing, the data will be ready to split into inputs (`X`) and targets (`y_grade` and `y_romantic`) for training the multi-task model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1d5c7b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading data from file\n",
    "df = pd.read_csv(CVS_PATH, delimiter=';')\n",
    "\n",
    "# This does one-hot encoding (for each column each category we will have column (e.g. sex_M, sex_F) that has either 0 or 1 value)\n",
    "df = pd.get_dummies(\n",
    "    df, \n",
    "    columns=COLUMNS_TO_CATEGORIZE,\n",
    "    prefix=COLUMNS_TO_CATEGORIZE\n",
    ").astype(int)\n",
    "\n",
    "# We use StandardScaler here, nothing special\n",
    "scaler = StandardScaler()\n",
    "df[COLUMNS_TO_NORMALIZE] = scaler.fit_transform(df[COLUMNS_TO_NORMALIZE])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7378ab5",
   "metadata": {},
   "source": [
    "Separate the features and targets:  \n",
    "- `X` contains all input features.  \n",
    "- `y_grade` is the regression target (final grade G3).  \n",
    "- `y_romantic` is the classification target (romantic status, 0/1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab0084a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X = df.drop(columns=['G3', 'romantic_yes', 'romantic_no']).values.astype(np.float32)\n",
    "y_grade = df['G3'].values.astype(np.float32).reshape(-1, 1)   # type: ignore\n",
    "y_romantic = df['romantic_yes'].values.astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e658298",
   "metadata": {},
   "source": [
    "Split the data into **train, validation, and test sets**:  \n",
    "- The first split separates out the test set (15% of the data).  \n",
    "- The second split divides the remaining data into training and validation sets (≈75% train, ≈15% validation).  \n",
    "This ensures we can train the model, validate during training, and evaluate on a held-out test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c065f88",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_trainval, X_test, y_grade_trainval, y_grade_test, y_rom_trainval, y_rom_test = train_test_split(\n",
    "    X, y_grade, y_romantic, test_size=0.15, random_state=42\n",
    ")\n",
    "X_train, X_val, y_grade_train, y_grade_val, y_rom_train, y_rom_val = train_test_split(\n",
    "    X_trainval, y_grade_trainval, y_rom_trainval, test_size=0.176, random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "115a9fc5",
   "metadata": {},
   "source": [
    "Create a **custom PyTorch Dataset** to handle our multi-task data:  \n",
    "\n",
    "- Returns a tuple `(x_data, y_grade, y_romantic)` for each sample.  \n",
    "- Ensures features are `float32` and targets are the correct types (`float32` for regression, `long` for classification).  \n",
    "- Works seamlessly with a `DataLoader` for batching."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "857e3ec3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class StudentDatasetPor(Dataset):\n",
    "    def __init__(self, X, y_grade, y_romantic):\n",
    "        self.X = torch.tensor(X, dtype=torch.float32)\n",
    "        self.y_grade = torch.tensor(y_grade, dtype=torch.float32)\n",
    "        self.y_romantic = torch.tensor(y_romantic, dtype=torch.long)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y_grade[idx], self.y_romantic[idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99cc55c2",
   "metadata": {},
   "source": [
    "Instantiate the **train, validation, and test datasets** using our custom `StudentDatasetPor` class.  \n",
    "These datasets will later be fed into `DataLoader`s for batching during training and evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fa8cdd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = StudentDatasetPor(X_train, y_grade_train, y_rom_train)\n",
    "val_dataset = StudentDatasetPor(X_val, y_grade_val, y_rom_val)\n",
    "test_dataset = StudentDatasetPor(X_test, y_grade_test, y_rom_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3fb942c",
   "metadata": {},
   "source": [
    "Define **training hyperparameters** and **network layer sizes**:  \n",
    "\n",
    "- `BATCH_SIZE`: Number of samples per batch  \n",
    "- `FIRST_NEURON_N`, `SECOND_NEURON_N`, `THIRD_NEURON_N`: Sizes of hidden layers in the shared MLP body  \n",
    "- `LEARNING_RATE`: Learning rate for the optimizer  \n",
    "- `EPOCHS`: Number of training iterations over the full dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b1bed4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 16\n",
    "FIRST_NEURON_N = 64\n",
    "SECOND_NEURON_N = 32\n",
    "THIRD_NEURON_N = 16\n",
    "LEARNING_RATE = 1e-3\n",
    "EPOCHS = 60"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "465847a7",
   "metadata": {},
   "source": [
    "Create **DataLoaders** to efficiently feed data into the model:  \n",
    "\n",
    "- `train_loader` shuffles the training data each epoch for better generalization.  \n",
    "- `val_loader` and `test_loader` do not shuffle, ensuring consistent evaluation.  \n",
    "- `BATCH_SIZE` controls how many samples are processed in each iteration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d86d264e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a093324a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class StudentMLP(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.shared = nn.Sequential(\n",
    "            nn.Linear(input_dim, FIRST_NEURON_N),\n",
    "            nn.BatchNorm1d(FIRST_NEURON_N),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.4),\n",
    "            nn.Linear(FIRST_NEURON_N, SECOND_NEURON_N),\n",
    "            nn.BatchNorm1d(SECOND_NEURON_N),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.4),\n",
    "            nn.Linear(SECOND_NEURON_N, THIRD_NEURON_N),  # optional\n",
    "            nn.BatchNorm1d(THIRD_NEURON_N),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        \n",
    "        self.grade_head = nn.Linear(THIRD_NEURON_N, 1)\n",
    "        \n",
    "        self.romantic_head = nn.Linear(THIRD_NEURON_N, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        features = self.shared(x)\n",
    "\n",
    "        grade_pred = torch.sigmoid(self.grade_head(features)) * 20\n",
    "        romantic_logit = self.romantic_head(features)\n",
    "\n",
    "        return grade_pred, romantic_logit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a823264",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model = StudentMLP(X.shape[1]).to(device)\n",
    "\n",
    "criterion_grade = nn.MSELoss()               \n",
    "criterion_romantic = nn.CrossEntropyLoss()\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbbb5226",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_losses = []\n",
    "val_losses = []\n",
    "train_grade_losses = []\n",
    "val_grade_losses = []\n",
    "train_romantic_losses = []\n",
    "val_romantic_losses = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6135884a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_loss_alpha(loss_grade, loss_romantic, alpha):\n",
    "    return loss_grade*alpha + (1-alpha)*loss_romantic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68e57cf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_val_model(alpha): \n",
    "    for epoch in range(EPOCHS):\n",
    "        model.train()\n",
    "        total_train_loss = 0\n",
    "        total_train_grade_loss = 0\n",
    "        total_train_romantic_loss = 0\n",
    "\n",
    "        for x_batch, y_grade_batch, y_romantic_batch in train_loader:\n",
    "            x_batch = x_batch.to(device)\n",
    "            y_grade_batch = y_grade_batch.to(device)\n",
    "            y_romantic_batch = y_romantic_batch.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            grade_pred, romantic_logits = model(x_batch)\n",
    "\n",
    "            loss_grade = criterion_grade(grade_pred, y_grade_batch)\n",
    "            loss_romantic = criterion_romantic(romantic_logits, y_romantic_batch)\n",
    "\n",
    "            if alpha is None:\n",
    "                loss = loss_grade + loss_romantic\n",
    "            else:\n",
    "                loss = calculate_loss_alpha(loss_grade=loss_grade, loss_romantic=loss_romantic, alpha=alpha)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_train_loss += loss.item() * x_batch.size(0)\n",
    "            total_train_grade_loss += loss_grade.item() * x_batch.size(0)\n",
    "            total_train_romantic_loss += loss_romantic.item() * x_batch.size(0)\n",
    "\n",
    "        total_train_loss /= len(train_loader.dataset) # type: ignore\n",
    "        total_train_grade_loss /= len(train_loader.dataset) # type: ignore\n",
    "        total_train_romantic_loss /= len(train_loader.dataset) # type: ignore\n",
    "        train_losses.append(total_train_loss)\n",
    "        train_grade_losses.append(total_train_grade_loss)\n",
    "        train_romantic_losses.append(total_train_romantic_loss)\n",
    "\n",
    "        # =======================\n",
    "        # VALIDATION\n",
    "        # =======================\n",
    "        model.eval()\n",
    "        total_val_loss = 0\n",
    "        total_val_grade_loss = 0\n",
    "        total_val_romantic_loss = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for x_batch, y_grade_batch, y_romantic_batch in val_loader:\n",
    "                x_batch = x_batch.to(device)\n",
    "                y_grade_batch = y_grade_batch.to(device)\n",
    "                y_romantic_batch = y_romantic_batch.to(device)\n",
    "\n",
    "                grade_pred, romantic_logits = model(x_batch)\n",
    "\n",
    "                loss_grade = criterion_grade(grade_pred, y_grade_batch)\n",
    "                loss_romantic = criterion_romantic(romantic_logits, y_romantic_batch)\n",
    "\n",
    "                if alpha is None:\n",
    "                    loss_val = loss_grade + loss_romantic\n",
    "                else:\n",
    "                    loss_val = calculate_loss_alpha(loss_grade, loss_romantic, alpha)\n",
    "\n",
    "                total_val_loss += loss_val.item() * x_batch.size(0)\n",
    "                total_val_grade_loss += loss_grade.item() * x_batch.size(0)\n",
    "                total_val_romantic_loss += loss_romantic.item() * x_batch.size(0)\n",
    "\n",
    "        total_val_loss /= len(val_loader.dataset) # type: ignore\n",
    "        total_val_grade_loss /= len(val_loader.dataset) # type: ignore\n",
    "        total_val_romantic_loss /= len(val_loader.dataset) # type: ignore\n",
    "        val_losses.append(total_val_loss)\n",
    "        val_grade_losses.append(total_val_grade_loss)\n",
    "        val_romantic_losses.append(total_val_romantic_loss)\n",
    "\n",
    "        print(\n",
    "            f\"Epoch [{epoch+1}/{EPOCHS}] | \"\n",
    "            f\"Train Loss: {total_train_loss:.4f} | \"\n",
    "            f\"Val Loss: {total_val_loss:.4f}\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fb54105",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_test(model, test_loader, device):\n",
    "    model.eval()\n",
    "\n",
    "    all_grade_preds = []\n",
    "    all_grade_true = []\n",
    "\n",
    "    all_romantic_preds = []\n",
    "    all_romantic_true = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for x_batch, y_grade_batch, y_romantic_batch in test_loader:\n",
    "            x_batch = x_batch.to(device)\n",
    "            y_grade_batch = y_grade_batch.to(device)\n",
    "            y_romantic_batch = y_romantic_batch.to(device)\n",
    "\n",
    "            grade_out, romantic_out = model(x_batch)\n",
    "\n",
    "            all_grade_preds.extend(grade_out.cpu().numpy().flatten())\n",
    "            all_grade_true.extend(y_grade_batch.cpu().numpy().flatten())\n",
    "\n",
    "            romantic_pred_labels = romantic_out.argmax(dim=1)\n",
    "            all_romantic_preds.extend(romantic_pred_labels.cpu().numpy())\n",
    "            all_romantic_true.extend(y_romantic_batch.cpu().numpy())\n",
    "\n",
    "\n",
    "    mae = mean_absolute_error(all_grade_true, all_grade_preds)\n",
    "\n",
    "    accuracy = accuracy_score(all_romantic_true, all_romantic_preds)\n",
    "\n",
    "    f1_yes = f1_score(all_romantic_true, all_romantic_preds, pos_label=1)\n",
    "\n",
    "    return {\n",
    "        \"grade_MAE\": mae,\n",
    "        \"romantic_accuracy\": accuracy,\n",
    "        \"romantic_f1_yes\": f1_yes\n",
    "    }\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c0e6023",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_val_model(0.03)\n",
    "print(evaluate_test(model=model, test_loader=test_loader, device=device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ce7d5e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"student_mlp_weights.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d100f7b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(train_losses, label=\"Train Total Loss\")\n",
    "plt.plot(val_losses, label=\"Val Total Loss\")\n",
    "plt.legend(); plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59f619da",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(train_grade_losses, label=\"Train Grade Loss\")\n",
    "plt.plot(val_grade_losses, label=\"Val Grade Loss\")\n",
    "plt.legend(); plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d52b2748",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(train_romantic_losses, label=\"Train Romantic Loss\")\n",
    "plt.plot(val_romantic_losses, label=\"Val Romantic Loss\")\n",
    "plt.legend(); plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e671e8c",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
